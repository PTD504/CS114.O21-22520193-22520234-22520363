{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8843724,"sourceType":"datasetVersion","datasetId":5322803},{"sourceId":8843857,"sourceType":"datasetVersion","datasetId":5322900}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BÁO CÁO BÀI TẬP THỰC HÀNH VÀ ĐỒ ÁN CUỐI KỲ\n\n- Dành cho lớp CS114.O21 và CS114.O21.KHCL","metadata":{}},{"cell_type":"markdown","source":"## THÔNG TIN NHÓM\n\n- Mỗi SV của nhóm điền các thông tin như bên dưới\n\n1. 22520193\n  - Họ và Tên: Phan Thanh Đăng\n  - Số buổi vắng: 0\n  - Số bài tập quá trình:\n  - Điểm WeCode: 2462\n\n2. 22520234\n  - Họ và Tên: Phùng Văn Đạt\n  - Số buổi vắng:\n  - Số bài tập quá trình:\n  - Điểm WeCode:\n\n3. 22520363\n  - Họ và Tên: Lê Văn Giáp\n  - Số buổi vắng:\n  - Số bài tập quá trình:\n  - Điểm Wecode:\n","metadata":{}},{"cell_type":"markdown","source":"## THÔNG TIN ĐỒ ÁN - THỰC HÀNH\n\nCác SV cần mô tả đầy đủ, càng chi tiết càng tốt về các bài thực hành đã làm để có cơ sở tính điểm đồ án và thực hành.\n\n1. Trang github của nhóm (đặt tên repos là mã lớp học-MSSV, ví dụ CS114.O21-MSSV): https://github.com/PTD504/CS114.O21-22520193-22520234-22520363\n\n2. Đồ án cuối kỳ: MotocycleClassification\n- Tổng số lượng ảnh đóng góp: 560.\n- Phương pháp thu thập ảnh: Thu thập từ nguồn Internet, cụ thể là kết quả tìm kiếm trên Microsoft Edge và trên Pinterest.com\n- Phương pháp làm sạch dữ liệu ảnh: Xử lý loại bỏ các ảnh bị trùng, loại bỏ những ảnh không hợp lệ (sai định dạng, những ảnh lỗi không mở được).\n- Post một số hình ảnh của các loại ở đây.\n- Phương pháp rút trích đặc trưng sử dụng: MobileNetV2\n- Thuật toán học được sử dụng: LogisticRegression, Convolutional Neural Network.\n- Framework, thư viện sử dụng: Keras, Tensorflow, Scikit-learn\n- Kết quả Accuracy: xxx (ví dụ điền 79.25%). Xếp hạng: 15 (theo danh sách file CS114.O21-O21.KHCL.ScoreBoard.csv)\n\n3. Danh sách các bài thực hành đã làm - điền thời điểm (ngày, giờ) nộp bài trên Classroom:\n- Thống kê dữ liệu (CS114.Tool.DatasetStat.ipynb): 6/06/2024\n- Tạo các splits (CS114.Tool.CreateSplit.ipynb): 10/06/2024\n- Hiển thị các ảnh (CS114.Tool.DatasetViz.ipynb): 10/06/2024\n- Ứng dụng Clustering (CS114.Clustering.ipynb):\n- Đánh giá Model (CS114.Evaluation.ipynb):\n\n4. Bài tập - Dự đoán điểm IT001\n- Mô tả về đặc trưng, các code đã dùng để rút trích đặc trưng, kết quả rút trích lưu trữ quản lý thế nào?\n- Thuật toán học, quá trình huấn luyện và thử nghiệm?\n\n5. Bài tập - Nhận dạng chữ số viết tay\n- Mô tả về dữ liệu đóng góp\n- Mô tả về đặc trưng và thuật toán học","metadata":{}},{"cell_type":"markdown","source":"## MÔ TẢ HỖ TRỢ CỦA CÁC CÔNG CỤ NHƯ CHATGPT, GEMINI, POE\n\n- Các SV mô tả vắn tắt việc dùng các công cụ để hỗ trợ cho việc thực hiện bài thực hành","metadata":{}},{"cell_type":"markdown","source":"## CODE CỦA ĐỒ ÁN MOTOCYCLECLASSIFICATION\n\n1. Tham khảo các bước\n  - https://keras.io/examples/vision/image_classification_from_scratch/\n\n2. Các lưu ý:\n  - Cần ghi rõ thông tin ngày cập nhật\n  - Đoạn code của người khác nếu sử dụng lại, phải ghi tham chiếu để phân biệt đâu là code của mình, đâu là code của người khác\n  - Cần phải có chú thích càng chi tiết càng tốt để code dễ đọc, dễ kiểm tra\n  - Notebook nộp bài phải là notebook đã chạy và giữ lại output\n  - Các hành vi gian lận sẽ bị trừng phạt rất nghiêm khắc","metadata":{}},{"cell_type":"markdown","source":"## CODE CHẠY CHO MỘT SPLIT\n\n- Các cells bên dưới sẽ phục vụ cho một split\n- Trước mỗi Code cell, nên có Text cell để giải thích","metadata":{}},{"cell_type":"markdown","source":"### Tải dữ liệu từ drive","metadata":{}},{"cell_type":"code","source":"!conda install -y gdown","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tải các thư mục chứa ảnh (Honda, Suzuki, Yamaha, VinFast, Others) và tạo file zip","metadata":{}},{"cell_type":"code","source":"!gdown 1ruKUAj-Z-D0qnE3hEZUXusJtaUqQefkz\n\n!gdown 1-2MmBIsxjy5EnxW9dz-f41tIEYGgxbE1\n\n!gdown 1-BX2D1FQLMYgp-l4bFVu7cIN9yMjJBUT\n\n!gdown 1-Dm6rZS9FN1lKmj-OK-uym0Wv07o92Yn\n\n!gdown 1lrUHUfMdzkFoceURpyAnAxp48rGrTIkJ\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /kaggle/working/Public","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Giải nén các file zip chứa ảnh vừa tải xuống","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\n\nfile_names = ['Honda', 'Suzuki', 'Yamaha' 'VinFast', 'Others']\n\nfor item in file_names:\n    zip_file_path = os.path.join('/kaggle/working', item + '.zip')\n    extrac_folder = os.path.join('/kaggle/working/Public', item)\n    \n    os.makedirs(extract_folder, exist_ok=True)\n    \n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_folder)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /kaggle/working/Honda/content/drive/MyDrive/Public/Honda /kaggle/working/Public\n!mv /kaggle/working/Suzuki/content/drive/MyDrive/Public/Suzuki /kaggle/working/Public\n!mv /kaggle/working/Yamaha/content/drive/MyDrive/Public/Yamaha /kaggle/working/Public\n!mv /kaggle/working/VinFast/content/drive/MyDrive/Public/VinFast /kaggle/working/Public\n!mv /kaggle/working/Others/content/drive/MyDrive/CS114/Public/Others /kaggle/working/Public","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm /kaggle/working/Honda.zip /kaggle/working/Suzuki.zip /kaggle/working/Yamaha.zip /kaggle/working/VinFast.zip /kaggle/working/Others.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Install and Import libraries","metadata":{}},{"cell_type":"code","source":"!pip install keras_tuner","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import các thư viện\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom keras import layers\nimport keras_tuner as kt\nfrom tensorflow import data as tf_data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tensorflow.keras.applications import MobileNetV2\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport os, csv, cv2, random as rd\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Các biến toàn cục\nCategory_map = {\n  'Others': 0,\n  'Honda': 1,\n  'Suzuki': 2,\n  'Yamaha': 3,\n  'VinFast': 4\n}\n\nbatch_size = 32\n\n# thư mục gốc chứa thư mục dữ liệu\n# szWorkingDir = 'drive/MyDrive/Public'\n# Đường dẫn đến các file csv chưa làm sạch\nszWorkingDir = '/kaggle/working/traintest-data'\n\n# dùng để hiển thị\nnNumImgsPerRow = 10\nnImgHeight = nImgWidth = 150","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hàm kiểm tra tính hợp lệ của ảnh\ndef is_valid_image(file_path):\n    try:\n        _ = Image.open(file_path)\n        return True\n    except:\n        return False\n\n# Hàm để đọc file CSV và loại bỏ các ảnh không hợp lệ\ndef load_and_filter_data_from_csv(csv_file, base_dir):\n    valid_file_paths = []\n    valid_labels = []\n\n    df = pd.read_csv(os.path.join(base_dir, csv_file))\n    for index, row in df.iterrows():\n        file_path = os.path.join(base_dir, row[0])\n        if is_valid_image(file_path):\n            valid_file_paths.append(file_path)\n            valid_labels.append(row[1])\n\n    return valid_file_paths, valid_labels\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Visualization\n\n- Mục đích để kiểm tra xem dữ liệu có sạch, chuẩn hoá chưa\n\n- Kết quả dự kiến <IMG SRC = 'https://editor.analyticsvidhya.com/uploads/762161_OSvbuPLy0PSM2nZ62SbtlQ.png'>","metadata":{}},{"cell_type":"code","source":"def show_images(filename):\n    df = pd.read_csv(os.path.join(base_dir, filename), names=['Image_path', 'CategoryID'])\n    # root = 'drive/MyDrive/CS114/Public'\n    # Category_map = {'Others': 0, 'Honda': 1, 'Suzuki': 2, 'Yamaha': 3, 'VinFast': 4}\n\n    for key, category_id in Category_map.items():\n        # Select excactly nNumImgsPerRow randomly\n        images_to_show = rd.sample(df[df['CategoryID'] == category_id]['Image_path'].tolist(), nNumImgsPerRow)\n\n        plt.figure(figsize=(20, 2))\n        plt.subplot(1, nNumImgsPerRow + 1, 1)\n        plt.text(0.5, 0.5, key, horizontalalignment = 'center', verticalalignment = 'center', fontsize = 12)\n        plt.axis('off')\n\n    # Show all selected images\n    for idx, image_path in enumerate(images_to_show):\n        try:\n            img = mpimg.imread(os.path.join(szWorkingDir, image_path))\n            img = cv2.resize(img, (nImgHeight, nImgWidth))\n        except Exception as e:\n            print(f'Error: {e}')\n            continue\n        plt.subplot(1, nNumImgsPerRow + 1, idx + 2)\n        plt.imshow(img)\n        plt.axis('off')\n    \n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chọn một file split bất kỳ để hiện thị một số ảnh\ncurSplit = 3\n\nszTrainFileName = \"MotocycleDataset-Splits-{}-Train.csv\".format(curSplit)\nszTestFileName = \"MotocycleDataset-Splits-{}-Test.csv\".format(curSplit)\n\nshow_images(szTrainFileName)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"code","source":"data_augmentation_layers = [\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n    layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)),\n]\n\ndef data_augmentation(images):\n    for layer in data_augmentation_layers:\n        images = layer(images)\n    return images\n\n# Function to apply `data_augmentation` to the training images.\ndef apply_data_augmentation(dataset):\n    dataset = dataset.map(\n        lambda x, y: (data_augmentation(tf.io.read_file(x)), y),\n        num_parallel_call=tf_data.AUTOTUNE,\n    )\n    \n    return dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate Dataset","metadata":{}},{"cell_type":"code","source":"def preprocess_image(file_path, label):\n    try:\n        image = tf.io.read_file(file_path)\n        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n        image = tf.image.resize(image, [224, 224])\n        image = image / 255.0  # Rescale the image\n        return image, label\n    except Exception as e:\n        print(f\"Error in preprocess_image for file {file_path}: {e}\")\n        pass\n\n# Tạo train dataset từ danh sách file_paths và categoryIDs\ndef make_dataset(file_paths, categoryIDs):\n    train_data = tf_data.Dataset.from_tensor_slices((file_paths, categoryIDs))\n    train_data = train_data.map(preprocess_image, num_parallel_calls=tf_data.AUTOTUNE)\n    \n    return train_data\n\ndef train_val_split(dataset):\n    # Shuffle the dataset to ensure randomness\n    X_train, X_val = dataset.shuffle(buffer_size=1000).split(0.8)\n    \n    # Apply augmentation on train data\n    X_train = apply_data_augmentation(X_train)\n    \n    X_train = X_train.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n    X_val = X_val.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n    \n    return X_train, X_val","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Extraction","metadata":{}},{"cell_type":"code","source":"# Tạo mô hình MobileNetV2 để rút trích đặc trưng\nbase_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')\n\n# Rút trích đặc trưng cho train data và validate data\ndef feature_extraction_tensorDataset(dataset):\n    image_dataset = dataset.map(lambda x, y: x)\n    X_features = base_model.predict(image_dataset)\n    y_labels = dataset.map(lambda x, y: y)\n    \n    X_features = np.array(X_features) # Dành cho LogisticRegression model\n    \n    return X_features, y_labels\n\n# Rút trích đặc trưng cho test data\ndef feature_extraction_dfDataset(dfDataset):\n    X_features = []\n    images = []\n    labels = []\n    \n    for index, row in dfDataset.iterrows():\n        image_path = row['Image_path']\n        image, _ preprocess_image(image_path, None)\n        if image:\n            images.append(image)\n            labels.append(row['CategoryID'])\n    \n    X_features = base_model.predict(images)\n    X_features = np.array(X_features) # Dành cho LogisticRegression model\n    \n    return X_features, labels\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Configuration","metadata":{}},{"cell_type":"code","source":"# Logistic Regression model\nclass MyModel:\n    def __init__(self):\n        # Choose 'lbfgs' for large datasets\n        self.solver = 'lbfgs'\n        # Ridge regularization\n        self.penalty = '12'\n        # Regularization strength (starting point)\n        self.C = 1.0\n        \n        # Other hyperparameters\n        self.tol = 1e-4 # Tolerance for stopping optimization\n        self.max_iter = 1000 # Maximum iterations (you already had this)\n        self.class_weight = None # No class weights by default\n        \n        self.model = LogisticRegression(solver=self.solver, penalty=self.penalty, C=self.C, tol=self.tol, max_iter=self.max_iter, class_weight=self.class_weight)\n    \n    def fit(X_train, y_train):\n        self.model.fit(X_train, y_train)\n    \n    def predict(data):\n        return self.model.predict(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"traintest_list = [\n    MotocycleDataset-Splits-1-Train.csv,\n    MotocycleDataset-Splits-1-Test.csv,\n    MotocycleDataset-Splits-2-Train.csv,\n    MotocycleDataset-Splits-2-Test.csv,\n    MotocycleDataset-Splits-3-Train.csv,\n    MotocycleDataset-Splits-3-Test.csv,\n    MotocycleDataset-Splits-4-Train.csv,\n    MotocycleDataset-Splits-4-Test.csv,\n    MotocycleDataset-Splits-5-Train.csv,\n    MotocycleDataset-Splits-5-Test.csv,\n]\n\nmodel = MyModel()\naccuracy_s = []\n\nfor i in range(1, 6):\n    print(f'DATASET {i}:')\n    szTrainFileName = \"MotocycleDataset-Splits-{}-Train.csv\".format(i)\n    szTestFileName = \"MotocycleDataset-Splits-{}-Test.csv\".format(i)\n    \n    train_file_paths, train_labels = load_and_filter_data_from_csv(szTrainFileName, base_dir)\n    test_file_paths, test_labels = load_and_filter_data_from_csv(szTestFileName, base_dir)\n    \n    train_data = make_dataset(train_file_paths, train_labels)\n    \n    X_train, X_val = train_val_split(train_data)\n    \n    X_train_features, y_train = feature_extraction_tensorDataset(X_train)\n    X_val_features, y_val = feature_extraction_tensorDataset(X_val)\n    \n    model.fit(X_train_features, y_train)\n    \n    y_pred = model.predict(X_val_features)\n    model_accuracy = accuracy_score(y_val, y_pred)\n    print(\"Model Accuracy:\", model_accuracy)\n    \n    test_data = pd.DataFrame({'Image_path': test_file_paths, 'CategoryID': test_labels})\n\n    test_features, test_labels = feature_extraction_dfDataset(test_data)\n    test_pred = model.predict(test_features)\n    \n    accuracy = accuracy_score(test_labels, test_pred)\n    accuracy_s.append(accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"import csv\nimport datetime\n\n# Define the submission data\ndate = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\ngroup_id = \"22520193-22520234-22520363\"\nsplit_id = curSplit\ndesc = \"Logistic Regression with new hyperparameters\"  # Replace with a brief description of the trial\n\n# Open the submission file in append mode\nwith open('CS114.O21-O21.KHCL.ScoreBoard.csv', 'a', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    \n    for index, accuracy in accuracy_s:\n        # Write the submission data\n        writer.writerow([date, group_id, index + 1, accuracy, desc])","metadata":{},"execution_count":null,"outputs":[]}]}